plot(x=distanceList,y=list)
##################function for getting vector of distance kernels for given data########
##returns a vector
filldist <-function(data)
{
list=c()
for(i in 1 : nrow(data)){
list[i]= GKerneldistance(data[i,])
}
return(list)
}
############################################################### kernel for Day ################
StarDay=as.Date(date)
GKernelDay <- function(Day2) {
DateDay2=as.Date(Day2)
daysdiff=((as.numeric(difftime(DateDay2,StarDay, units = "days")))%%365)
result = exp((- (daysdiff)^2)/(h_date)^2)
return(result)
}
############picking nice smoothing factor h_day###########
dayDistList=c()
daylist<-c()
for(i in 1 : nrow(st)){
dayDistList <- append(dayDistList,(as.numeric(difftime(as.Date(st[i,]$date),StarDay, units = "days"))%%365))
daylist<-append(daylist,GKernelDay(st[i,]$date))
}
#plotting the dependency of day kernels values on the distance between days
plot(x=dayDistList,y=daylist)
##########################################
##function for getting vector of day kernels for given data and day
##returns a vector
fillDay <-function(data)
{
daylist=c()
for(i in 1 : nrow(data)){
daylist[i]= GKernelDay(data[i,]$date)
}
return(daylist)
}
##################
############ kernel for Hours ################
GKernelHours <- function(Hour2,StarHour) {
first <- as.POSIXct(paste("2022-01-01 ",StarHour))
second <- as.POSIXct(paste("2022-01-01 ",Hour2))
# result = exp((- (as.numeric(difftime(Hour2,StarHour, units = "hours")))^2)/(2*h_time)^2)
result = exp((- (as.numeric( difftime(first, second,units = "hours")))^2)/(h_time)^2)
return(result)
}
##################This code to pick smoothing factor for hours#########
hourKernellistForH<-c()
############picking nice smoothing factor h_day###########
hoursDistList=c()
StarHourH=times[3]
StarHourHTime <- as.POSIXct(paste("2022-01-01 ",StarHourH))
for(i in 1 : nrow(st)){
currentTime <- as.POSIXct(paste("2022-01-01 ",st[i,]$time))
hoursDistList <- append(hoursDistList,(as.numeric( abs(difftime(currentTime, StarHourHTime,units = "hours")))))
hourKernellistForH<-append(hourKernellistForH,GKernelHours(st[i,]$time,StarHourH))
}
#plotting the dependency of day kernels values on the distance between days
plot(x=hoursDistList,y=hourKernellistForH)
######################################
#################### Sum and Multiply ####################
finalsum= c() #sum of kernels
mult1= list() #multiplications of kernels
forecastsum<-c()
forecastmult<-c()
#timelist=c(1:nrow(st))
daylist<-c()
for(i in 1 : length(times)){
StarHour=times[i]
#filtering
#print(StarHour)
stfiltered <- filter(st, date < StarDay & time<StarHour)
daylist=fillDay(stfiltered)
distlist=filldist(stfiltered)
#list[i]= GKerneldistance(stations[i,])
#daylist[i]= GKernelDay(temps[i,]$date)
timelist=c(1:nrow(stfiltered))
for(k in 1 : nrow(stfiltered)){
timelist[k]= GKernelHours(stfiltered[k,]$time,StarHour)
}
#timelist[i]= GKernelHours(st[i,]$time)
#sum1[i]= as.numeric(daylist[i])  + as.numeric(list[i]) + as.numeric(timelist[i])
#mult1[i]=as.numeric(daylist[i])  * as.numeric(list[i]) * as.numeric(timelist[i])
#finalsum<-as.numeric(timelist)+as.numeric(sumDistDay)
finalsum<-distlist+timelist+daylist
mult1=distlist*timelist*daylist
forecastsum[i]=sum(finalsum*stfiltered$air_temperature)/sum(finalsum)
forecastmult[i]=sum(mult1*stfiltered$air_temperature)/sum(mult1)
#print(timelist)
}
print(forecastsum)
print(forecastmult)
#plot(y = forecastsum,x = times, type="o", ylim=c(0,10),xlim=c(0,24))
plot(forecastsum, type="o", xlab = "Time", ylab = "Temperature forecast", xaxt="n",ylim=c(0,10),col="blue")
axis(1, at=1:length(times), labels = times)
#matplot(y = forecastsum,x = times, ylim=c(0,8),xlim=c(0,24))
points(forecastmult, type="o", xlab = "Time", ylab = "Temperature by MultiKernel", xaxt="n",col="green")
#axis(1, at=1:length(times), labels = times)
legend(5, 10, c("sum", "mult"), col = c(4,3), text.col = "red", pch = c(3, 4))
#Plot of kernel value as a function of distance.
#First three graph need not be shown but we select smoothing factor based on them.
#Final 2 graphs should. x distances between points and on y kernel value.
#plot(y = forecastsum,x = times, type="o", ylim=c(0,10),xlim=c(0,24))
plot(forecastsum, type="o", xlab = "Time", ylab = "Temperature forecast", xaxt="n",ylim=c(0,25),col="blue")
axis(1, at=1:length(times), labels = times)
#matplot(y = forecastsum,x = times, ylim=c(0,8),xlim=c(0,24))
points(forecastmult, type="o", xlab = "Time", ylab = "Temperature by MultiKernel", xaxt="n",col="green")
#axis(1, at=1:length(times), labels = times)
legend(5, 10, c("sum", "mult"), col = c(4,3), text.col = "red", pch = c(3, 4))
#plotting the dependency of distance kernels values on physical distance
plot(x=distanceList,y=list)
#plotting the dependency of distance kernels values on physical distance
plot(x=distanceList,y=list)
################Assignment 1########################333
#install.packages("geosphere")
library(geosphere)
#install.packages("dplyr")
library(dplyr)
################preparations########################
set.seed(1234567890)
library(geosphere)
stations <- read.csv("stationsutf8.csv")
temps <- read.csv("temps50k.csv")
st <- merge(stations,temps,by="station_number")
h_distance <- 100000# We picked this because the graph the graph looked nice, otherwise consider it based on like average distance between stations
h_date <- 10
h_time <-2
# we chose manually a width that gives large kernel values to closer points and small values to distant points.
# a=latitude  b= longitude
a <- 59.335 # The point to predict (up to the students)- Stockholm
b <- 18.063
date <- "2013-08-04" # The date to predict (up to the students)
times <- c("04:00:00", "06:00:00", "08:00:00", "10:00:00", "12:00:00","14:00:00","16:00:00","18:00:00","20:00:00","22:00:00","24:00:00")
temp <- vector(length=length(times))
# Studentsâ€™ code here
#########function to calculate Gaussian kernel for distance ############
#########takes a row from st table as an input
StarStation= c(a,b)
GKerneldistance <- function(station2) {
coordinatesStation2=c(station2$latitude,station2$longitude)
distance <- distHaversine(StarStation,coordinatesStation2)
result = exp(- (distance/h_distance)^2 )
return(result)
}
############Implement a list in day hour that distances for day and hours to select smoothing factor
#Basically make the other kernel sections look more like day section.
############picking nice smoothing factor h_distance###########
distanceList=c()
list<-c()
for(i in 1 : nrow(st)){
distanceList <- append(distanceList,distHaversine(StarStation,c(st[i,]$latitude,st[i,]$longitude)))
list[i]= GKerneldistance(st[i,])
#print(GKerneldistance(st[i,]))
}
#plotting the dependency of distance kernels values on physical distance
plot(x=distanceList,y=list)
##################function for getting vector of distance kernels for given data########
##returns a vector
filldist <-function(data)
{
list=c()
for(i in 1 : nrow(data)){
list[i]= GKerneldistance(data[i,])
}
return(list)
}
############################################################### kernel for Day ################
StarDay=as.Date(date)
GKernelDay <- function(Day2) {
DateDay2=as.Date(Day2)
daysdiff=((as.numeric(difftime(DateDay2,StarDay, units = "days")))%%365)
result = exp((- (daysdiff)^2)/(h_date)^2)
return(result)
}
############picking nice smoothing factor h_day###########
dayDistList=c()
daylist<-c()
for(i in 1 : nrow(st)){
dayDistList <- append(dayDistList,(as.numeric(difftime(as.Date(st[i,]$date),StarDay, units = "days"))%%365))
daylist<-append(daylist,GKernelDay(st[i,]$date))
}
#plotting the dependency of day kernels values on the distance between days
plot(x=dayDistList,y=daylist)
##function for getting vector of day kernels for given data and day
##returns a vector
fillDay <-function(data)
{
daylist=c()
for(i in 1 : nrow(data)){
daylist[i]= GKernelDay(data[i,]$date)
}
return(daylist)
}
##################
############ kernel for Hours ################
GKernelHours <- function(Hour2,StarHour) {
first <- as.POSIXct(paste("2022-01-01 ",StarHour))
second <- as.POSIXct(paste("2022-01-01 ",Hour2))
# result = exp((- (as.numeric(difftime(Hour2,StarHour, units = "hours")))^2)/(2*h_time)^2)
result = exp((- (as.numeric( difftime(first, second,units = "hours")))^2)/(h_time)^2)
return(result)
}
##################This code to pick smoothing factor for hours#########
hourKernellistForH<-c()
############picking nice smoothing factor h_day###########
hoursDistList=c()
StarHourH=times[3]
StarHourHTime <- as.POSIXct(paste("2022-01-01 ",StarHourH))
for(i in 1 : nrow(st)){
currentTime <- as.POSIXct(paste("2022-01-01 ",st[i,]$time))
hoursDistList <- append(hoursDistList,(as.numeric( abs(difftime(currentTime, StarHourHTime,units = "hours")))))
hourKernellistForH<-append(hourKernellistForH,GKernelHours(st[i,]$time,StarHourH))
}
#plotting the dependency of day kernels values on the distance between days
plot(x=hoursDistList,y=hourKernellistForH)
finalsum= c() #sum of kernels
mult1= list() #multiplications of kernels
forecastsum<-c()
forecastmult<-c()
#timelist=c(1:nrow(st))
daylist<-c()
for(i in 1 : length(times)){
StarHour=times[i]
#filtering
#print(StarHour)
stfiltered <- filter(st, date < StarDay & time<StarHour)
daylist=fillDay(stfiltered)
distlist=filldist(stfiltered)
#list[i]= GKerneldistance(stations[i,])
#daylist[i]= GKernelDay(temps[i,]$date)
timelist=c(1:nrow(stfiltered))
for(k in 1 : nrow(stfiltered)){
timelist[k]= GKernelHours(stfiltered[k,]$time,StarHour)
}
#timelist[i]= GKernelHours(st[i,]$time)
#sum1[i]= as.numeric(daylist[i])  + as.numeric(list[i]) + as.numeric(timelist[i])
#mult1[i]=as.numeric(daylist[i])  * as.numeric(list[i]) * as.numeric(timelist[i])
#finalsum<-as.numeric(timelist)+as.numeric(sumDistDay)
finalsum<-distlist+timelist+daylist
mult1=distlist*timelist*daylist
forecastsum[i]=sum(finalsum*stfiltered$air_temperature)/sum(finalsum)
forecastmult[i]=sum(mult1*stfiltered$air_temperature)/sum(mult1)
#print(timelist)
}
print(forecastsum)
print(forecastmult)
#plot(y = forecastsum,x = times, type="o", ylim=c(0,10),xlim=c(0,24))
plot(forecastsum, type="o", xlab = "Time", ylab = "Temperature forecast", xaxt="n",ylim=c(0,25),col="blue")
axis(1, at=1:length(times), labels = times)
#matplot(y = forecastsum,x = times, ylim=c(0,8),xlim=c(0,24))
points(forecastmult, type="o", xlab = "Time", ylab = "Temperature by MultiKernel", xaxt="n",col="green")
#axis(1, at=1:length(times), labels = times)
legend(5, 10, c("sum", "mult"), col = c(4,3), text.col = "red", pch = c(3, 4))
#plot(y = forecastsum,x = times, type="o", ylim=c(0,10),xlim=c(0,24))
plot(forecastsum, type="o", xlab = "Time", ylab = "Temperature forecast", xaxt="n",ylim=c(0,25),col="blue")
axis(1, at=1:length(times), labels = times)
#matplot(y = forecastsum,x = times, ylim=c(0,8),xlim=c(0,24))
points(forecastmult, type="o", xlab = "Time", ylab = "Temperature by MultiKernel", xaxt="n",col="green")
#axis(1, at=1:length(times), labels = times)
legend(5, 5, c("sum", "mult"), col = c(4,3), text.col = "red", pch = c(3, 4))
plot(forecastsum, type="o", xlab = "Time", ylab = "Temperature forecast", ylim=c(0,25),col="blue")
axis(1, at=1:length(times), labels = times)
points(forecastmult, type="o", xlab = "Time", ylab = "Temperature by MultiKernel", col="green")
legend(5, 5, c("sum", "mult"), col = c(4,3), text.col = "red", pch = c(3, 4))
plot(forecastsum, type="o", xlab = "Time", ylab = "Temperature forecast", xaxt="n",ylim=c(0,25),col="blue")
axis(1, at=1:length(times), labels = times)
points(forecastmult, type="o", xlab = "Time", ylab = "Temperature by MultiKernel", xaxt="n",col="green")
legend(5, 5, c("sum", "mult"), col = c(4,3), text.col = "red", pch = c(3, 4))
plot(forecastsum, type="o", xlab = "Time", xaxt="n",ylim=c(0,25),col="blue")
axis(1, at=1:length(times), labels = times)
points(forecastmult, type="o", xlab = "Time", xaxt="n",col="green")
legend(5, 5, c("sum", "mult"), col = c(4,3), text.col = "red", pch = c(3, 4))
plot(forecastsum, type="o", xlab = "Time", ylab = "Temperature forecast", xaxt="n",ylim=c(0,25),col="blue")
axis(1, at=1:length(times), labels = times)
points(forecastmult, type="o", xlab = "Time", ylab = "Temperature by MultiKernel", xaxt="n",col="green")
legend(5, 5, c("sum", "mult"), col = c(4,3), text.col = "red", pch = c(3, 4))
plot(forecastsum, type="o", xlab = "Time", ylab = "Temperature forecast", xaxt="n",ylim=c(0,25),col="blue")
axis(1, at=1:length(times), labels = times)
points(forecastmult, type="o", xlab = "Time", xaxt="n",col="green")
legend(5, 5, c("sum", "mult"), col = c(4,3), text.col = "red", pch = c(3, 4))
setwd("C:/TDDE01ML/labs/lab3")
# Lab 3 block 1 of 732A99/TDDE01/732A68 Machine Learning
# Author: jose.m.pena@liu.se
# Made for teaching purposes
library(kernlab)
set.seed(1234567890)
data(spam)
#random order????
foo <- sample(nrow(spam))
spam <- spam[foo,]
#scaling
spam[,-58]<-scale(spam[,-58])
#splitting
tr <- spam[1:3000, ] #train
va <- spam[3001:3800, ] #valid
trva <- spam[1:3800, ] # train and valid
te <- spam[3801:4601, ] #test
by <- 0.3
err_va <- NULL
for(i in seq(by,5,by)){
#calculating svm with different regularization terms
filter <- ksvm(type~.,data=tr,kernel="rbfdot",kpar=list(sigma=0.05),C=i,scaled=FALSE)
#making a prediction
mailtype <- predict(filter,va[,-58])
#confusion matrix
t <- table(mailtype,va[,58])
#storing misclassification into the vector
err_va <-c(err_va,(t[1,2]+t[2,1])/sum(t))
}
#we get the C from err_va which gives the lowest MCR to use it in the filters further
err_va
#Smaller data sets than trva. And no usage of test
filter0 <- ksvm(type~.,data=tr,kernel="rbfdot",kpar=list(sigma=0.05),C=which.min(err_va)*by,scaled=FALSE)
mailtype <- predict(filter0,va[,-58])
t <- table(mailtype,va[,58])
err0 <- (t[1,2]+t[2,1])/sum(t)
err0
#Smaller training dataset and no usage of validation dataset
filter1 <- ksvm(type~.,data=tr,kernel="rbfdot",kpar=list(sigma=0.05),C=which.min(err_va)*by,scaled=FALSE)
mailtype <- predict(filter1,te[,-58])
t <- table(mailtype,te[,58])
err1 <- (t[1,2]+t[2,1])/sum(t)
err1
#filter returned to user. Why? Largest usage of available data while still keeping some seperate for testing.
filter2 <- ksvm(type~.,data=trva,kernel="rbfdot",kpar=list(sigma=0.05),C=which.min(err_va)*by,scaled=FALSE)
mailtype <- predict(filter2,te[,-58])
t <- table(mailtype,te[,58])
err2 <- (t[1,2]+t[2,1])/sum(t)
err2
#Training dataset contained test dataset
filter3 <- ksvm(type~.,data=spam,kernel="rbfdot",kpar=list(sigma=0.05),C=which.min(err_va)*by,scaled=FALSE)
mailtype <- predict(filter3,te[,-58])
t <- table(mailtype,te[,58])
err3 <- (t[1,2]+t[2,1])/sum(t)
err3
# Lab 3 block 1 of 732A99/TDDE01/732A68 Machine Learning
# Author: jose.m.pena@liu.se
# Made for teaching purposes
library(kernlab)
set.seed(1234567890)
data(spam)
#random order????
foo <- sample(nrow(spam))
spam <- spam[foo,]
#scaling
spam[,-58]<-scale(spam[,-58])
#splitting
tr <- spam[1:3000, ] #train
va <- spam[3001:3800, ] #valid
trva <- spam[1:3800, ] # train and valid
te <- spam[3801:4601, ] #test
by <- 0.3
err_va <- NULL
for(i in seq(by,5,by)){
#calculating svm with different regularization terms
filter <- ksvm(type~.,data=tr,kernel="rbfdot",kpar=list(sigma=0.05),C=i,scaled=FALSE)
#making a prediction
mailtype <- predict(filter,va[,-58])
#confusion matrix
t <- table(mailtype,va[,58])
#storing misclassification into the vector
err_va <-c(err_va,(t[1,2]+t[2,1])/sum(t))
}
#we get the C from err_va which gives the lowest MCR to use it in the filters further
#Smaller data sets than trva. And no usage of test
filter0 <- ksvm(type~.,data=tr,kernel="rbfdot",kpar=list(sigma=0.05),C=which.min(err_va)*by,scaled=FALSE)
mailtype <- predict(filter0,va[,-58])
t <- table(mailtype,va[,58])
err0 <- (t[1,2]+t[2,1])/sum(t)
err0
#Smaller training dataset and no usage of validation dataset
filter1 <- ksvm(type~.,data=tr,kernel="rbfdot",kpar=list(sigma=0.05),C=which.min(err_va)*by,scaled=FALSE)
mailtype <- predict(filter1,te[,-58])
t <- table(mailtype,te[,58])
err1 <- (t[1,2]+t[2,1])/sum(t)
err1
#filter returned to user. Why? Largest usage of available data while still keeping some seperate for testing.
filter2 <- ksvm(type~.,data=trva,kernel="rbfdot",kpar=list(sigma=0.05),C=which.min(err_va)*by,scaled=FALSE)
mailtype <- predict(filter2,te[,-58])
t <- table(mailtype,te[,58])
err2 <- (t[1,2]+t[2,1])/sum(t)
err2
#Training dataset contained test dataset
filter3 <- ksvm(type~.,data=spam,kernel="rbfdot",kpar=list(sigma=0.05),C=which.min(err_va)*by,scaled=FALSE)
mailtype <- predict(filter3,te[,-58])
t <- table(mailtype,te[,58])
err3 <- (t[1,2]+t[2,1])/sum(t)
err3
library(neuralnet)
#####Task 1#####
set.seed(1234567890)
Var <- runif(500, 0, 10)
mydata <- data.frame(Var, Sin=sin(Var))
tr <- mydata[1:25,] # Training
te <- mydata[26:500,] # Test
# Random initialization of the weights in the interval [-1, 1]
# Question: It is only for task 4 or we should have it in other tasks too?!
winit <-  runif(25, -1, 1)# Your code here
nn <- neuralnet(Sin~Var, data=tr, hidden=10, startweights=winit)
# Plot of the training data (black), test data (blue), and predictions (red)
plot(tr, cex=2)
points(te, col = "blue", cex=1)
points(te[,1],predict(nn,te), col="red", cex=1)
#Comment. The model is good. But we see a slight error in valley of the curve.
#####Task 2#####
winit <-  runif(25, -1, 1)
linear <- function(x) x
nnLinear <- neuralnet(Sin~Var, data=tr, hidden=10, act.fct = linear, startweights=winit)
winit <-  runif(25, -1, 1)
softplus <- function(x) log(1 + exp(x))
nnSoftPlus <- neuralnet(Sin~Var, data=tr, hidden=10, act.fct = softplus)
#there is a problem with plugging in max function into neuralnet because it is not differentiable
#see https://stackoverflow.com/questions/34532878/package-neuralnet-in-r-rectified-linear-unit-relu-activation-function
#so we use workaround
#install.packages('sigmoid')
library(sigmoid)
#relu()
# you do not need to implement this just explain why you cannot implement max
winit <-  runif(25, -1, 1)
nnReLu <- neuralnet(Sin~Var, data=tr, hidden=10, act.fct =relu, startweights=winit)
#plotting
#linear
plot(tr, cex=2)
points(te, col = "blue", cex=1)
points(te[,1],predict(nnLinear,te), col="red", cex=1)
#It is a straight line. A bit to high up, 0.25.
#softplus
plot(tr, cex=2)
points(te, col = "blue", cex=1)
points(te[,1],predict(nnSoftPlus,te), col="red", cex=1)
#ReLu
plot(tr, cex=2)
points(te, col = "blue", cex=1)
points(te[,1],predict(nnReLu,te), col="red", cex=1)
#the linear activation function gives a straight line and can't approximate non-linear sine function
# The soft plus fits as the best
# the softplus and relu are non-linear activation functions that considers unknown variables to the difference of the linear
#activation function.
#"Number of hidden nodes affect complexity and how well it fits to the data. "
#For some reason max(0,a) doesn't work.
#Max(0,x) doesn't work because it is not differentiable function. It can't be derived at zero.
#aka the left and right side of the derivative are  not identical, which leads to problems and error for rstudio,
#there are functions with workarounds but we need not implement them.
##### Task 3 #####
set.seed(1234567890)
Var <- runif(500, 0, 50)
mydata <- data.frame(Var, Sin=sin(Var))
tr <- mydata[1:25,] # Training
te <- mydata[26:500,] # Test
#plot(mydata)
#plot(tr)
plot(te)
# Random initialization of the weights in the interval [-1, 1]
winit <-  runif(25, -1, 1)
nntask3 <- neuralnet(Sin~Var, data=tr, hidden=10, startweights=winit)
# Plot of the training data (black), test data (blue), and predictions (red)
plot(tr, cex=2)
points(te, col = "blue", cex=1)
points(te[,1],predict(nntask3,te), col="red", cex=1)
library(Hmisc)
minor.tick(ny = 5,  tick.ratio = 1) #line seems to be exactly at -0.347
# The prediction up to 10 variables is similar to task 1 (quite good), but for the rest it gets worse.
##### Task 4 #####
print(nntask3$weights)
#print(nn$weights)
# Look at some (large) x value (1 is the bias)
x <- c(1, 50)
print(x)
# Print the hidden units
hidden <- c(1, 1/(1+exp(-x %*% nntask3$weights[[1]][[1]])))
print(hidden)
# Print the prediction
prediction <- hidden %*% nntask3$weights[[1]][[2]]
print(prediction)
# Print the weights
print(nntask3$weights)
plot(nntask3)
# It can be seen that
#If we increase training data in proportion to test data the model improves.
#if we increase the number of nodes in the hidden layer the model improves.
#We can see that the graph converges to the value -0.347 . We think this happens because the model is not complex enough considering
#number of hidden nodes and amount of training data to fit to changes beyond 10 variables.
#Weights are attached to inputs and convey the importance of that input in predicting final output
# Sigmoid function approaches 1 if hidden node times weight for the hidden is a positive value and 0 if the opposite
# So the final output will be close to 1*(sum of weights corresponding to non-zero hidden units).
# Demonstration: as there only 5 non-zero hidden units + bias, only bias and weights ## 4,6,7,9,10 are taken into consideration
# and they sum up exactly to the value the prediction converges to
# [4,] -1.2142028
#[6,]  0.6218254
#[7,] -1.2120181
#[9,] -1.2250588
#[10,]  2.1300140
summm<- sum(-1.2142028, 0.6218254,-1.2120181,-1.2250588,2.1300140,0.5520328)
#exactly -0.3474075
plot(nntask3)
##### Task 5 #####
#predict x from sin(x)
set.seed(1234567890)
Var <- runif(500, 0, 10)
mydata <- data.frame(Var, Sin=sin(Var))
# Random initialization of the weights in the interval [-1, 1]
winit <-  runif(25, -1, 1)
#NN that predicts x from sin(x) on the whole dataset (all point are training)
nnreversed <- neuralnet(Var~Sin, data=mydata, hidden=10, startweights=winit,threshold = 0.1)
reversedprediction=predict(nnreversed,mydata)
# Plot of the training data (black), test data (blue), and predictions (red)
plot(mydata, cex=2) #black dots = training data
points(reversedprediction,mydata[,2],col="red", cex=1)
#prediction is very bad, not matching the data at all
#because we are trying to get a linear prediction from non-linear function of sine. for example at the point of sin(x)=0 there may be 4 different x.
