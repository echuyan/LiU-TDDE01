a=1
#Problem 3. Monte Carlo method
rm(list=ls()) # this is to clear all previous data
a = 0; b = 0.5; c = 1; n = 20
set.seed(1) # fix random generator seed (same random simulations each time)
x=runif(n, min=0, max=b) # generate n uniform in the rectangle for x
y=runif(n, min=0, max=c) # generate n uniform in the rectangle for y
y_curve=(1/sqrt(2*3.14))*exp(-x^2/2) # the normal curve
f_n=sum(y<y_curve) # count how many in the region I
area_I=(b-a)*c*f_n/n # compute the area of I
f_n
rm(list=ls()) # this is to clear all previous data
a = 0; b = 0.5; c = 1; n = 20
set.seed(1) # fix random generator seed (same random simulations each time)
x=runif(n, min=0, max=b) # generate n uniform in the rectangle for x
y=runif(n, min=0, max=c) # generate n uniform in the rectangle for y
y_curve=(1/sqrt(2*3.14))*exp(-x^2/2) # the normal curve
f_n=sum(y<y_curve) # count how many in the region I
area_I=(b-a)*c*f_n/n # compute the area of I
n1 = 50
set.seed(1)
x=runif(n1, min=0, max=b)
y=runif(n1, min=0, max=c)
y_curve=(1/sqrt(2*3.14))*exp(-x^2/2)
f_n=sum(y<y_curve)
area_I=(b-a)*c*f_n/n1
#Problem 3. Monte Carlo method
#generate n uniform random variables in the rectangle (a, b) × (0, c)
rm(list=ls()) # this is to clear all previous data
a = 0; b = 0.5; c = 1; n = 20
set.seed(1) # fix random generator seed (same random simulations each time)
x=runif(n, min=0, max=b) # generate n uniform in the rectangle for x
y=runif(n, min=0, max=c) # generate n uniform in the rectangle for y
y_curve=(1/sqrt(2*3.14))*exp(-x^2/2) # the normal curve
f_n=sum(y<y_curve) # count how many in the region I
area_I=(b-a)*c*f_n/n # compute the area of I
#n=50
n1 = 50
set.seed(1)
x1=runif(n1, min=0, max=b)
y1=runif(n1, min=0, max=c)
f_n1=sum(y1<y_curve)
area_I1=(b-a)*c*f_n1/n1
#n=50
n1 = 50
set.seed(1)
x1=runif(n1, min=0, max=b)
y1=runif(n1, min=0, max=c)
f_n1=sum(y1<y_curve)
area_I1=(b-a)*c*f_n1/n1
y1=runif(n1, min=0, max=c)
f_n1=sum(y1<y_curve)
n1 = 50
set.seed(1)
x1=runif(n1, min=0, max=b)
y1=runif(n1, min=0, max=c)
f_n1=sum(y1<y_curve)
area_I1=(b-a)*c*f_n1/n1
#n=500
n2 = 500
set.seed(1)
x2=runif(n2, min=0, max=b)
y2=runif(n2, min=0, max=c)
f_n2=sum(y2<y_curve)
area_I2=(b-a)*c*f_n2/n2
n3 = 10000
set.seed(1)
x3=runif(n3, min=0, max=b)
y3=runif(n3, min=0, max=c)
f_n3=sum(y3<y_curve)
area_I3=(b-a)*c*f_n3/n3
#(i) We first generate 1000 samples (each sample size is 1) from Bin(16, 0.3).
rm(list=ls()) # this is to clear all previous data
n = 16; p = 0.3
set.seed(1) # fix random generator seed (same random simulations each time)
x = rbinom(1000,n,p) # one can think of x as 1000 samples (each sample size is 1)
#For each sample one can estimate p by using p. ˆ So 1000 samples give us 1000 estimated pˆ
phat = x/n # these are 1000 estimated values of p
lower_lim = phat - 1.96*sqrt(phat*(1-phat)/n) # this is a vector with 1000 values
upper_lim = phat + 1.96*sqrt(phat*(1-phat)/n) # this is a vector with 1000 values
#Since it is 95%, there should be around 950 such intervals containing the real value p = 0.3, and around 50 not containing the real value.
#Now we count how many such intervals not containing p = 0.3 :
missing = sum(lower_lim > p) + sum(upper_lim < p)
#Problem 5. CI using normal approximations
#In this problem we will construct confidence intervals when the population is a Binomial random variable Bin(n, p).
#In order to use normal approximation, it is required that n is large enough such that np ≥ 10 and n(1 − p) ≥ 10.
#It is interesting to see what happens if n is not that large, so we will construct 95% confidence interval of p using normal approximations for both
#large and not so large n.
#(i) We first generate 1000 samples (each sample size is 1) from Bin(16, 0.3).
rm(list=ls()) # this is to clear all previous data
n = 16; p = 0.3
set.seed(1) # fix random generator seed (same random simulations each time)
x = rbinom(1000,n,p) # one can think of x as 1000 samples (each sample size is 1)
#For each sample one can estimate p by using p. ˆ So 1000 samples give us 1000 estimated pˆ
phat = x/n # these are 1000 estimated values of p
#The corresponding 1000 (95%) confidence intervals are:
lower_lim = phat - 1.96*sqrt(phat*(1-phat)/n) # this is a vector with 1000 values
upper_lim = phat + 1.96*sqrt(phat*(1-phat)/n) # this is a vector with 1000 values
#Now we have 1000 such 95% confidence intervals Ip=(lower_lim, upper_lim).
#Since it is 95%, there should be around 950 such intervals containing the real value p = 0.3, and around 50 not containing the real value.
#Now we count how many such intervals not containing p = 0.3 :
missing = sum(lower_lim > p) + sum(upper_lim < p)
#where ‘missing’ gives you the number of intervals not containing the real value p = 0.3.
#Compare this number with the expected number 50 (is it far away from 50 or very close to 50? why?)
#Problem 5. CI using normal approximations
#In this problem we will construct confidence intervals when the population is a Binomial random variable Bin(n, p).
#In order to use normal approximation, it is required that n is large enough such that np ≥ 10 and n(1 − p) ≥ 10.
#It is interesting to see what happens if n is not that large, so we will construct 95% confidence interval of p using normal approximations for both
#large and not so large n.
#(i) We first generate 1000 samples (each sample size is 1) from Bin(16, 0.3).
rm(list=ls()) # this is to clear all previous data
n = 16; p = 0.3
set.seed(1) # fix random generator seed (same random simulations each time)
x = rbinom(1000,n,p) # one can think of x as 1000 samples (each sample size is 1)
#For each sample one can estimate p by using p. ˆ So 1000 samples give us 1000 estimated pˆ
phat = x/n # these are 1000 estimated values of p
#The corresponding 1000 (95%) confidence intervals are:
lower_lim = phat - 1.96*sqrt(phat*(1-phat)/n) # this is a vector with 1000 values
upper_lim = phat + 1.96*sqrt(phat*(1-phat)/n) # this is a vector with 1000 values
#Now we have 1000 such 95% confidence intervals Ip=(lower_lim, upper_lim).
#Since it is 95%, there should be around 950 such intervals containing the real value p = 0.3, and around 50 not containing the real value.
#Now we count how many such intervals not containing p = 0.3 :
missing = sum(lower_lim > p) + sum(upper_lim < p)
#where ‘missing’ gives you the number of intervals not containing the real value p = 0.3.
#Compare this number with the expected number 50 (is it far away from 50 or very close to 50? why?)
#Problem 5. CI using normal approximations
#In this problem we will construct confidence intervals when the population is a Binomial random variable Bin(n, p).
#In order to use normal approximation, it is required that n is large enough such that np ≥ 10 and n(1 − p) ≥ 10.
#It is interesting to see what happens if n is not that large, so we will construct 95% confidence interval of p using normal approximations for both
#large and not so large n.
#(i) We first generate 1000 samples (each sample size is 1) from Bin(16, 0.3).
rm(list=ls()) # this is to clear all previous data
n = 16; p = 0.3
set.seed(1) # fix random generator seed (same random simulations each time)
x = rbinom(1000,n,p) # one can think of x as 1000 samples (each sample size is 1)
#For each sample one can estimate p by using p. ˆ So 1000 samples give us 1000 estimated pˆ
phat = x/n # these are 1000 estimated values of p
#The corresponding 1000 (95%) confidence intervals are:
lower_lim = phat - 1.96*sqrt(phat*(1-phat)/n) # this is a vector with 1000 values
upper_lim = phat + 1.96*sqrt(phat*(1-phat)/n) # this is a vector with 1000 values
#Now we have 1000 such 95% confidence intervals Ip=(lower_lim, upper_lim).
#Since it is 95%, there should be around 950 such intervals containing the real value p = 0.3, and around 50 not containing the real value.
#Now we count how many such intervals not containing p = 0.3 :
missing = sum(lower_lim > p) + sum(upper_lim < p)
n1 = 80; p1 = 0.3
set.seed(1)
y = rbinom(1000,n1,p1)
phaty = y/n1
lower_limy = phaty - 1.96*sqrt(phaty*(1-phaty)/n1)
upper_limy = phaty + 1.96*sqrt(phaty*(1-phaty)/n1)
#Since it is 95%, there should be around 950 such intervals containing the real value p = 0.3, and around 50 not containing the real value.
#Now we count how many such intervals not containing p = 0.3 :
missingy = sum(lower_limy > p1) + sum(upper_limy < p1)
x = c(4.4, 3.9, 4.0, 4.0, 3.5, 4.1, 2.3, 4.7, 1.7, 4.9, 1.7, 4.6, 3.4, 4.3, 1.7, 3.9,
3.7, 3.1, 4.0, 1.8, 4.1, 1.8, 3.2, 1.9, 4.6, 2.0, 4.5, 3.9, 4.3, 2.3, 3.8, 1.9,
4.6, 1.8, 4.7, 1.8, 4.6, 1.9, 3.5, 4.0, 3.7, 3.7, 4.3, 3.6, 3.8, 3.8, 3.8, 2.5,
4.5, 4.1, 3.7, 3.8, 3.4, 4.0, 2.3, 4.4, 4.1, 4.3, 3.3, 2.0, 4.3, 2.9, 4.6, 1.9,
3.6, 3.7, 3.7, 1.8, 4.6, 3.5, 4.0, 3.7, 1.7, 4.6, 1.7, 4.0, 1.8, 4.4, 1.9, 4.6,
2.9, 3.5, 2.0, 4.3, 1.8, 4.1, 1.8, 4.7, 4.2, 3.9, 4.3, 1.8, 4.5, 2.0, 4.2, 4.4,
4.1, 4.1, 4.0, 4.1, 2.7, 4.6, 1.9, 4.5, 2.0, 4.8, 4.1)
y = c(78, 74, 68, 76, 80, 84, 50, 93, 55, 76, 58, 74, 75, 80, 56, 80, 69, 57, 90, 42,
91, 51, 79, 53, 82, 51, 76, 82, 84, 53, 86, 51, 85, 45, 88, 51, 80, 49, 82, 75,
73, 67, 68, 86, 72, 75, 75, 66, 84, 70, 79, 60, 86, 71, 67, 81, 76, 83, 76, 55,
73, 56, 83, 57, 71, 72, 77, 55, 75, 73, 70, 83, 50, 95, 51, 82, 54, 83, 51, 80,
78, 81, 53, 89, 44, 78, 61, 73, 75, 73, 76, 55, 86, 48, 77, 73, 70, 88, 75, 83,
61, 78, 61, 81, 51, 80, 79)
x = c(4.4, 3.9, 4.0, 4.0, 3.5, 4.1, 2.3, 4.7, 1.7, 4.9, 1.7, 4.6, 3.4, 4.3, 1.7, 3.9,
3.7, 3.1, 4.0, 1.8, 4.1, 1.8, 3.2, 1.9, 4.6, 2.0, 4.5, 3.9, 4.3, 2.3, 3.8, 1.9,
4.6, 1.8, 4.7, 1.8, 4.6, 1.9, 3.5, 4.0, 3.7, 3.7, 4.3, 3.6, 3.8, 3.8, 3.8, 2.5,
4.5, 4.1, 3.7, 3.8, 3.4, 4.0, 2.3, 4.4, 4.1, 4.3, 3.3, 2.0, 4.3, 2.9, 4.6, 1.9,
3.6, 3.7, 3.7, 1.8, 4.6, 3.5, 4.0, 3.7, 1.7, 4.6, 1.7, 4.0, 1.8, 4.4, 1.9, 4.6,
2.9, 3.5, 2.0, 4.3, 1.8, 4.1, 1.8, 4.7, 4.2, 3.9, 4.3, 1.8, 4.5, 2.0, 4.2, 4.4,
4.1, 4.1, 4.0, 4.1, 2.7, 4.6, 1.9, 4.5, 2.0, 4.8, 4.1)
y = c(78, 74, 68, 76, 80, 84, 50, 93, 55, 76, 58, 74, 75, 80, 56, 80, 69, 57, 90, 42,
91, 51, 79, 53, 82, 51, 76, 82, 84, 53, 86, 51, 85, 45, 88, 51, 80, 49, 82, 75,
73, 67, 68, 86, 72, 75, 75, 66, 84, 70, 79, 60, 86, 71, 67, 81, 76, 83, 76, 55,
73, 56, 83, 57, 71, 72, 77, 55, 75, 73, 70, 83, 50, 95, 51, 82, 54, 83, 51, 80,
78, 81, 53, 89, 44, 78, 61, 73, 75, 73, 76, 55, 86, 48, 77, 73, 70, 88, 75, 83,
61, 78, 61, 81, 51, 80, 79)
#(i) We first plot y against x in order to see if there is a linear relation between them:
plot(x,y)
#We can even find the correlation coefficient ρX,Y :
correlation=cor(x,y)
regre=lm(y~x)
summary(regre)
View(regre)
res=regre$residuals # to get the residuals
plot(x,res) # plot of the residuals vs x
hist(res) # histogram of the residuals
x = c(41, 41, 42, 43, 54, 53, 57, 58, 63, 66, 67, 67, 67, 68, 69, 70, 70, 70, 70,
72, 73, 75, 75, 76, 76, 78, 79, 81, 85, 86, 86, 88)
y = c(1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0,
0, 0, 0, 0, 0, 0, 0)
x = c(41, 41, 42, 43, 54, 53, 57, 58, 63, 66, 67, 67, 67, 68, 69, 70, 70, 70, 70,
72, 73, 75, 75, 76, 76, 78, 79, 81, 85, 86, 86, 88)
y = c(1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0,
0, 0, 0, 0, 0, 0, 0)
logregre=glm(y~x,family=binomial())
summary(logregre)
logregre=glm(y~x,family=binomial())
summary(logregre)
# Lab 3 block 1 of 732A99/TDDE01/732A68 Machine Learning
# Author: jose.m.pena@liu.se
# Made for teaching purposes
library(kernlab)
set.seed(1234567890)
data(spam)
#random order????
foo <- sample(nrow(spam))
spam <- spam[foo,]
#scaling
spam[,-58]<-scale(spam[,-58])
#splitting
tr <- spam[1:3000, ] #train
va <- spam[3001:3800, ] #valid
trva <- spam[1:3800, ] # train and valid
te <- spam[3801:4601, ] #test
by <- 0.3
err_va <- NULL
for(i in seq(by,5,by)){
#calculating svm with different regularization terms
filter <- ksvm(type~.,data=tr,kernel="rbfdot",kpar=list(sigma=0.05),C=i,scaled=FALSE)
#making a prediction
mailtype <- predict(filter,va[,-58])
#confusion matrix
t <- table(mailtype,va[,58])
#storing misclassification into the vector
err_va <-c(err_va,(t[1,2]+t[2,1])/sum(t))
}
#we get the C from err_va which gives the lowest MCR to use it in the filters further
#Smaller data sets than trva. And no usage of test
filter0 <- ksvm(type~.,data=tr,kernel="rbfdot",kpar=list(sigma=0.05),C=which.min(err_va)*by,scaled=FALSE)
mailtype <- predict(filter0,va[,-58])
t <- table(mailtype,va[,58])
err0 <- (t[1,2]+t[2,1])/sum(t)
err0
#Smaller training dataset and no usage of validation dataset
filter1 <- ksvm(type~.,data=tr,kernel="rbfdot",kpar=list(sigma=0.05),C=which.min(err_va)*by,scaled=FALSE)
mailtype <- predict(filter1,te[,-58])
t <- table(mailtype,te[,58])
err1 <- (t[1,2]+t[2,1])/sum(t)
err1
#filter returned to user. Why? Largest usage of available data while still keeping some seperate for testing.
filter2 <- ksvm(type~.,data=trva,kernel="rbfdot",kpar=list(sigma=0.05),C=which.min(err_va)*by,scaled=FALSE)
mailtype <- predict(filter2,te[,-58])
t <- table(mailtype,te[,58])
err2 <- (t[1,2]+t[2,1])/sum(t)
err2
#Training dataset contained test dataset
filter3 <- ksvm(type~.,data=spam,kernel="rbfdot",kpar=list(sigma=0.05),C=which.min(err_va)*by,scaled=FALSE)
mailtype <- predict(filter3,te[,-58])
t <- table(mailtype,te[,58])
err3 <- (t[1,2]+t[2,1])/sum(t)
err3
# Questions
# 1. Which filter do we return to the user ? filter0, filter1, filter2 or filter3? Why?
#POSSIBLE ANSWER: filter 2 because obtained on largerst dataset and tested on unseen data
#!!!!QUESTION!!! Is it correct?
# 2. What is the estimate of the generalization error of the filter returned to the user? err0, err1, err2 or err3? Why?
#POSSIBLEANSWER: we reject err0
#Take into account: unseen data used, unseen data for C (reg term), size of the error and size of the dataset =>
# => err1 or err 2. But err1 is obtained on a smaller dataset (tr) and is a bit bigger than err2. Possibly err2
#!!!!QUESTION
# 3. Implementation of SVM predictions.
#install.packages("kernlab")
library(kernlab)
sv<-alphaindex(filter3)[[1]] #The index of the resulting support vectors in the data matrix. Note that this index refers to the pre-processed data (after the possible effect of na.omit and subset)
co<-coef(filter3)[[1]] #The corresponding coefficients times the training labels.
inte<- - b(filter3) #The negative intercept.
#k<-0
for(i in 1:10){ # We produce predictions for just the first 10 points in the dataset.
k2<-0
for(j in 1:length(sv)){
#k2<- # Your code here
x= as.vector(spam[sv[j],-58])
names(x)<-NULL
x<-as.numeric(x)
y= as.vector(spam[i,-58])
names(y)<-NULL
y<-as.numeric(y)
RBFkernel= rbfkernel(x,y)
#cat("RBF: ",RBFkernel)
k2 <- k2+co[j]*RBFkernel
# cat("k2 ",k2)
}
k2<-k2+inte
# cat("k2 after intercept: ",k2)
sig<-sign(k2)
#cat("sig: ",sig)
k<-c(k,sig) # Your code here)
}
install.packages("kernlab")
library(kernlab)
install.packages("kernlab")
install.packages("kernlab")
library(kernlab)
install.packages("kernlab")
install.packages("kernlab")
install.packages("kernlab")
install.packages("kernlab")
library(kernlab)
sv<-alphaindex(filter3)[[1]] #The index of the resulting support vectors in the data matrix. Note that this index refers to the pre-processed data (after the possible effect of na.omit and subset)
co<-coef(filter3)[[1]] #The corresponding coefficients times the training labels.
inte<- - b(filter3) #The negative intercept.
#k<-0
for(i in 1:10){ # We produce predictions for just the first 10 points in the dataset.
k2<-0
for(j in 1:length(sv)){
#k2<- # Your code here
x= as.vector(spam[sv[j],-58])
names(x)<-NULL
x<-as.numeric(x)
y= as.vector(spam[i,-58])
names(y)<-NULL
y<-as.numeric(y)
RBFkernel= rbfkernel(x,y)
#cat("RBF: ",RBFkernel)
k2 <- k2+co[j]*RBFkernel
# cat("k2 ",k2)
}
k2<-k2+inte
# cat("k2 after intercept: ",k2)
sig<-sign(k2)
#cat("sig: ",sig)
k<-c(k,sig) # Your code here)
}
library(neuralnet)
#####Task 1#####
set.seed(1234567890)
Var <- runif(500, 0, 10)
mydata <- data.frame(Var, Sin=sin(Var))
tr <- mydata[1:25,] # Training
te <- mydata[26:500,] # Test
# Random initialization of the weights in the interval [-1, 1]
# Question: It is only for task 4 or we should have it in other tasks too?!
winit <-  runif(25, -1, 1)# Your code here
nn <- neuralnet(Sin~Var, data=tr, hidden=10, startweights=winit)
# Plot of the training data (black), test data (blue), and predictions (red)
plot(tr, cex=2)
points(te, col = "blue", cex=1)
points(te[,1],predict(nn,te), col="red", cex=1)
#Comment. The model is good. But we see a slight error in valley of the curve.
#Why.
#####Task 2#####
winit <-  runif(25, -1, 1)
linear <- function(x) x
nnLinear <- neuralnet(Sin~Var, data=tr, hidden=10, act.fct = linear, startweights=winit)
winit <-  runif(25, -1, 1)
softplus <- function(x) log(1 + exp(x))
nnSoftPlus <- neuralnet(Sin~Var, data=tr, hidden=10, act.fct = softplus)
#there is a problem with plugging in max function into neuralnet because it is not differentiable
#see https://stackoverflow.com/questions/34532878/package-neuralnet-in-r-rectified-linear-unit-relu-activation-function
#so we use workaround
#install.packages('sigmoid')
library(sigmoid)
#relu()
# you do not need to implement this just explain why you cannot implement max
winit <-  runif(25, -1, 1)
nnReLu <- neuralnet(Sin~Var, data=tr, hidden=10, act.fct =relu, startweights=winit)
#plotting
#linear
plot(tr, cex=2)
points(te, col = "blue", cex=1)
points(te[,1],predict(nnLinear,te), col="red", cex=1)
#It is a straight line. A bit to high up, 0.25.
#softplus
plot(tr, cex=2)
points(te, col = "blue", cex=1)
points(te[,1],predict(nnSoftPlus,te), col="red", cex=1)
#ReLu
plot(tr, cex=2)
points(te, col = "blue", cex=1)
points(te[,1],predict(nnReLu,te), col="red", cex=1)
# The soft plus fits as the best
# the softplus and relu are non-linear activation functions that considers unknown variables to the difference of the linear
#activation function.
#"Number of hidden nodes affect complexity and how well it fits to the data. "
#For some reason max(0,a) doesn't work.
#Max(0,x) doesn't work because it is not differentiable function.
#aka the left and right side of the derivative are  not identical, which leads to problems and error for rstudio,
#there are functions with workarounds but we need not implement them.
##### Task 3 #####
set.seed(1234567890)
Var <- runif(500, 0, 50)
mydata <- data.frame(Var, Sin=sin(Var))
tr <- mydata[1:25,] # Training
te <- mydata[26:500,] # Test
#plot(mydata)
#plot(tr)
plot(te)
# Random initialization of the weights in the interval [-1, 1]
winit <-  runif(25, -1, 1)
nntask3 <- neuralnet(Sin~Var, data=tr, hidden=10, startweights=winit)
# Plot of the training data (black), test data (blue), and predictions (red)
plot(tr, cex=2)
points(te, col = "blue", cex=1)
points(te[,1],predict(nntask3,te), col="red", cex=1)
# The prediction up to 10 variables is similar to task 1, but for the rest it gets worse.
#If we increase training data in proportion to test data the model improves.
#if we increase the number of nodes in the hidden layer the model improves.
##### Task 4 #####
print(nntask3$weights)
#print(nn$weights)
#We can see that the graph converges to the value -0.5 . We think this happens because the model is not complex enough considering
#number of hidden nodes and amount of training data to fit to changes beyond 10 variables.
#Weights are attached to inputs and convey the importance of that input in predicting final output
#
plot(nntask3)
set.seed(1234567890)
Var <- runif(500, 0, 10)
mydata <- data.frame(Var, Sin=sin(Var))
tr <- mydata[1:25,] # Training
te <- mydata[26:500,] # Test
# Random initialization of the weights in the interval [-1, 1]
winit <-  runif(25, -1, 1)
#NN that predicts x from sin(x) on the whole dataset (all point are training)
nnreversed <- neuralnet(Var~Sin, data=mydata, hidden=10, startweights=winit,threshold = 0.1)
reversedprediction=predict(nnreversed,tr)
# Plot of the training data (black), test data (blue), and predictions (red)
plot(tr, cex=2)
points(tr[,1],reversedprediction, col="red", cex=1)
# Plot of the training data (black), test data (blue), and predictions (red)
plot(tr, cex=2,ylim=10)
# Plot of the training data (black), test data (blue), and predictions (red)
plot(tr, cex=2,ylim=c(0,8))
points(tr[,1],reversedprediction, col="red", cex=1)
points(reversedprediction,tr[,1],col="red", cex=1)
# Plot of the training data (black), test data (blue), and predictions (red)
plot(tr, cex=2,ylim=c(0,8))
points(reversedprediction,tr[,1],col="red", cex=1)
# Plot of the training data (black), test data (blue), and predictions (red)
plot(tr, cex=2,ylim=c(0,8))
points(reversedprediction,tr[,2],col="red", cex=1)
# Plot of the training data (black), test data (blue), and predictions (red)
plot(tr, cex=2)
points(reversedprediction,tr[,2],col="red", cex=1)
reversedprediction=predict(nnreversed,mydata)
plot(mydata, cex=2) #black dots = training data
points(reversedprediction,mydata[,2],col="red", cex=1)
setwd("C:/TDDE01ML/labs/lab2")
##################################################Preparations##########################################
tecator=read.csv("tecator.csv")
#REMOVE UNNECESSARY COLUMNS FIRST
tecator$Protein=c()
tecator$Moisture=c()
tecator$Sample=c()
#dividing the train and test
set.seed(12345)
n=nrow(tecator)
id=sample(1:n, floor(n*0.5))
train=tecator[id,]
test=tecator[-id,]
#model based on training data
linearModel=lm(Fat~., data=train)
sum=summary(linearModel)
######################################################Task 1.###########################################
#MSE for train and test
MSEtrain=mean(sum$residuals^2)
#calculate MSE for test data set
predictionTest=predict(linearModel,newdata=test,interval = "prediction")
MSEtest=mean((test$Fat-predictionTest)^2)
######################################################Task 2.############################################
#cost function - see the report
######################################################Task 3.############################################
library(glmnet)
library(dplyr)
#creating lasso regression model
x=as.matrix(train%>%select(-Fat))
y=as.matrix(train%>%select(Fat))
LASSOmodel=glmnet(x, y, alpha=1)
plot(LASSOmodel, xvar = "lambda")
######################################################Task 4.############################################
#Ridge regression and  plot of lambda
ridgeRegressionModel=glmnet(x, y, alpha=0)
plot(ridgeRegressionModel, xvar="lambda")
######################################################Task 5.##########################################################
#cross validation
modelCrossValidation <- cv.glmnet(x=x, y=y, alpha=1, family="gaussian")
plot(modelCrossValidation)
#lambda min is 0.05745
print(modelCrossValidation$lambda.min)
#build a model with minLambda and see how many variables are there
LASSOmodelOptimalLambda=glmnet(x, y, alpha=1,lambda=modelCrossValidation$lambda.min)
print(LASSOmodelOptimalLambda$beta)
View(LASSOmodelOptimalLambda)
