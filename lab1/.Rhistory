a=1
#----------------------------lab1. assignment2. linear and ridge regression---------------------------
#setwd("/Users/eli/Desktop/Machine learning/Lab 1/")
#loading the dataset
parkinson=read.csv("parkinsons.csv")
setwd("C:/TDDE01ML/labs/lab1")
#----------------------------lab1. assignment2. linear and ridge regression---------------------------
#setwd("/Users/eli/Desktop/Machine learning/Lab 1/")
#loading the dataset
parkinson=read.csv("parkinsons.csv")
#REMOVE UNNECESSARY COLUMNS FIRST
parkinson$subject.=c()
parkinson$sex=c()
parkinson$test_time=c()
parkinson$age=c()
parkinson$total_UPDRS=c()
#choosing training and test sets
set.seed(12345)
n=nrow(parkinson)
id=sample(1:n, floor(n*0.6))
train=parkinson[id,]
test=parkinson[-id,]
#scaling
library(caret)
scaler=preProcess(train)
trainS=predict(scaler,train)
testS=predict(scaler,test)
#Compute a linear regression model from the training data, estimate training
#and test MSE and comment on which variables contribute significantly to the model
#no intercept is needed in the modelling
fit = lm(motor_UPDRS~.-1 , data=trainS)
sum=summary(fit)
#calculate MSE for train dataset
MSEtrain=mean(sum$residuals^2)
#run model on test dataset
fitTest=predict(fit,testS,interval = "prediction")
#calculate MSE for test dataset
MSEtest=mean((testS$motor_UPDRS-fitTest)^2)
#comment on which variables contribute significantly to the model.
# Shimmer.DDA
# Shimmer.APQ3
# we scaled the data => resulting coefficients can be compared and we can compare absolute values
#??????question?????????? (significant contribution) - should some other variables be picked? How to define a threshold?
#answer : look at p-values and ***
#CHECK IN GOOGLE
summary(fit)
#Implement 4 following functions
#3a) Log-likelihood function that for a given parameter vector ğœ½ and
#dispersion ğœ computes the log-likelihood function log ğ‘ƒ(ğ‘‡|ğœ½, ğœ) for
#the stated model and the training data
#Going by that motor_UPDRS is a normal distribution as mentioned in 1.
#Formula log-likelihood F(ğœ½)=log(L(ğœ½))=sum(over i=1 to n) log(fi(yi|ğœ½))
#??????question??????????: is the set of ğœ½we are trying to optimize equal to the vector of coefficients in LR model   - YES
vectorTheta = fit$coefficients
names(vectorTheta) <- NULL
dispersionSigma = summary(fit)$sigma
logLikelihood<-function(theta,sigma){
#population size n
n<-length(theta)
#print(n)
actualValue<-trainS$motor_UPDRS[1:n] #y in the formula.
#print(actualValue)
#print(length(parkinson$motor_UPDRS))
#ERROR!
#Expected value, estimated by using the average value.
#ev=mean(theta)
#changed it to transpose of the in parametr theta to match p44 book.
predictedTheta=theta
#print(ev)
#Done here because when calculations were done in "sum" in the formula, errors appeared.
x=(predictedTheta-actualValue)^2 #is vector of
#print("printing x")
#print(x)
#ERROR!
#Formula
result=((-1*n/2)*log(2*pi,base=exp(1))-(n/2)*log(sigma^2,base=exp(1))-1/(2*sigma^2)*sum(x, na.rm=FALSE))
return (result)
}
print(logLikelihood(vectorTheta,dispersionSigma))
logLik(fit)
#3b) Ridge function that for given vector ğœ½ğœ½, scalar ğœğœ and scalar ğœ†ğœ† uses function from 3a and adds up a Ridge penalty ğœ†â€–ğœ½ğœ½â€–2 to the minus loglikelihood
mylambda=1
myridge<-function(theta,sigma,lambda){
# print(theta)
#print(sigma)
#print(lambda)
ridgePenalty=lambda*sum(theta^2)
#print(ridgePenalty)
# print(logLikelihood(theta,sigma ))
result =  -logLikelihood(theta,sigma )+ ridgePenalty
# print(result)
return (result)
}
print(myridge(vectorTheta,dispersionSigma,mylambda))
# 3c) RidgeOpt function that depends on scalar ğœ† , uses function from 3b
#and function optim() with method=â€BFGSâ€ to find the optimal ğœ½ and ğœ for the given ğœ†.
#ANSWER: initiate with 0 and then compare the result of our custom prediction with true values
par1 <- rep(c(0),each=16)
sigmaPar=1
par1 <- append(par1,sigmaPar)
#par1 <- append(par1,dispersionSigma)
#par1 <- append(par1,lambda)
myRidgeOpt=function(lambdaIn) {
#result <- optim(par1,fn=myridge,theta=par1[1:64],sigma=par1[65],lambda=par1[66],method="BFGS")
#resultTheta <- optim(par1,fn=myridge,sigma=dispersionSigma,lambda=lambdaIn,method="BFGS")
#resultSigma<- optim(dispersionSigma,fn=myridge,theta=par1,lambda=lambdaIn,method="BFGS")
result<- optim(par1,fn=myridge,sigma=sigmaPar,lambda=lambdaIn,method="BFGS")
#WE SHOULD OPTIMIZE THEM TOGETHER
#result <- resultTheta$par
#result=append(result,resultSigma$par)
return (result)
}
optimal=myRidgeOpt(mylambda)
print(optimal)
View(fit)
View(fitTest)
View(parkinson)
c <- c(1,2,3,4,5)
c <- c[1:3]
#----------------------------lab1. assignment2. linear and ridge regression---------------------------
#setwd("/Users/eli/Desktop/Machine learning/Lab 1/")
#loading the dataset
parkinson=read.csv("parkinsons.csv")
#REMOVE UNNECESSARY COLUMNS FIRST
parkinson$subject.=c()
parkinson$sex=c()
parkinson$test_time=c()
parkinson$age=c()
parkinson$total_UPDRS=c()
#choosing training and test sets
set.seed(12345)
n=nrow(parkinson)
id=sample(1:n, floor(n*0.6))
train=parkinson[id,]
test=parkinson[-id,]
#scaling
library(caret)
scaler=preProcess(train)
trainS=predict(scaler,train)
testS=predict(scaler,test)
#Compute a linear regression model from the training data, estimate training
#and test MSE and comment on which variables contribute significantly to the model
#no intercept is needed in the modelling
fit = lm(motor_UPDRS~.-1 , data=trainS)
sum=summary(fit)
#calculate MSE for train dataset
MSEtrain=mean(sum$residuals^2)
#run model on test dataset
fitTest=predict(fit,testS,interval = "prediction")
#calculate MSE for test dataset
MSEtest=mean((testS$motor_UPDRS-fitTest)^2)
#comment on which variables contribute significantly to the model.
# Shimmer.DDA
# Shimmer.APQ3
# we scaled the data => resulting coefficients can be compared and we can compare absolute values
#??????question?????????? (significant contribution) - should some other variables be picked? How to define a threshold?
#answer : look at p-values and ***
#CHECK IN GOOGLE
summary(fit)
#Implement 4 following functions
#3a) Log-likelihood function that for a given parameter vector ğœ½ and
#dispersion ğœ computes the log-likelihood function log ğ‘ƒ(ğ‘‡|ğœ½, ğœ) for
#the stated model and the training data
#Going by that motor_UPDRS is a normal distribution as mentioned in 1.
#Formula log-likelihood F(ğœ½)=log(L(ğœ½))=sum(over i=1 to n) log(fi(yi|ğœ½))
#??????question??????????: is the set of ğœ½we are trying to optimize equal to the vector of coefficients in LR model   - YES
vectorTheta = fit$coefficients
names(vectorTheta) <- NULL
dispersionSigma = summary(fit)$sigma
logLikelihood<-function(theta,sigma){
#population size n
n<-length(theta)
#print(n)
actualValue<-trainS$motor_UPDRS[1:n] #y in the formula.
#print(actualValue)
#print(length(parkinson$motor_UPDRS))
#ERROR!
#Expected value, estimated by using the average value.
#ev=mean(theta)
#changed it to transpose of the in parametr theta to match p44 book.
predictedTheta=theta
#print(ev)
#Done here because when calculations were done in "sum" in the formula, errors appeared.
x=(predictedTheta-actualValue)^2 #is vector of
#print("printing x")
#print(x)
#ERROR!
#Formula
result=((-1*n/2)*log(2*pi,base=exp(1))-(n/2)*log(sigma^2,base=exp(1))-1/(2*sigma^2)*sum(x, na.rm=FALSE))
return (result)
}
print(logLikelihood(vectorTheta,dispersionSigma))
logLik(fit)
#3b) Ridge function that for given vector ğœ½ğœ½, scalar ğœğœ and scalar ğœ†ğœ† uses function from 3a and adds up a Ridge penalty ğœ†â€–ğœ½ğœ½â€–2 to the minus loglikelihood
mylambda=1
myridge<-function(theta,sigma,lambda){
# print(theta)
#print(sigma)
#print(lambda)
ridgePenalty=lambda*sum(theta^2)
#print(ridgePenalty)
# print(logLikelihood(theta,sigma ))
result =  -logLikelihood(theta,sigma )+ ridgePenalty
# print(result)
return (result)
}
print(myridge(vectorTheta,dispersionSigma,mylambda))
# 3c) RidgeOpt function that depends on scalar ğœ† , uses function from 3b
#and function optim() with method=â€BFGSâ€ to find the optimal ğœ½ and ğœ for the given ğœ†.
#ANSWER: initiate with 0 and then compare the result of our custom prediction with true values
par1 <- rep(c(0),each=16)
sigmaPar=1
par1 <- append(par1,sigmaPar)
#par1 <- append(par1,dispersionSigma)
#par1 <- append(par1,lambda)
myRidgeOpt=function(lambdaIn) {
#result <- optim(par1,fn=myridge,theta=par1[1:64],sigma=par1[65],lambda=par1[66],method="BFGS")
#resultTheta <- optim(par1,fn=myridge,sigma=dispersionSigma,lambda=lambdaIn,method="BFGS")
#resultSigma<- optim(dispersionSigma,fn=myridge,theta=par1,lambda=lambdaIn,method="BFGS")
result<- optim(par1,fn=myridge,sigma=sigmaPar,lambda=lambdaIn,method="BFGS")
#WE SHOULD OPTIMIZE THEM TOGETHER
#result <- resultTheta$par
#result=append(result,resultSigma$par)
return (result)
}
optimal=myRidgeOpt(mylambda)
print(optimal)
#??????question??????????
#how can we prove that the result is indeed optimal and not erroneous in any way?
#can we build a ridge regression model using glmnet library?
#3d) Df function that for a given scalar ğœ† computes the degrees of freedom
#of the Ridge model based on the training data
install.packages("glmnet")
library(glmnet)
x <- data.matrix(trainS[, c('Jitter...', 'Jitter.Abs.', 'Jitter.RAP', 'Jitter.PPQ5','Jitter.DDP','Shimmer','Shimmer.dB.','Shimmer.APQ3',
'Shimmer.APQ5','Shimmer.APQ11','Shimmer.DDA','NHR','HNR','RPDE','DFA','PPE')])
y<-trainS$motor_UPDRS
ridgeModel=glmnet(x,y,alpha=0)
y_predicted <- predict(ridgeModel, s = 1, newx = x)
View(ridgeModel)
View(y_predicted)
lambdaIn=1
DF=function(lambdaIn) {
#(df)= âˆ‘ni=1(di)2(di)2+Î»
#plan:
#get A^T*A-lambda*I, I - identity matrix
#get eigenvalues from that equation
#calculate degrees of freedom as (df)= âˆ‘j=1,p (dj)^2/((dj)^2+Î»), where dj^2 are eigenvalues and p is a number of variables
#question: will this approach help us find degrees of freedom or are there better approaches to use?
A<-data.matrix(trainS)
nxn_matrix=A%*%t(A)
#print(dim(nxn_matrix))
n<-nrow(nxn_matrix)
print(n)
identitymatrix<-diag(n)
y<-det(data.matrix(nxn_matrix-lambdaIn*identitymatrix))
print(y)
#print(eigen(y, only.values = TRUE))
df<-residuals(fit)
#answer: YES!
}
print(DF(mylambda))
A<-data.matrix(trainS)
nxn_matrix=A%*%t(A)
#print(dim(nxn_matrix))
n<-nrow(nxn_matrix)
print(n)
identitymatrix<-diag(n)
install.packages("psych")
library("psych")
print(tr(identitymatrix))
print(tr(identitymatrix))
#----------------------------lab1. assignment2. linear and ridge regression---------------------------
#setwd("/Users/eli/Desktop/Machine learning/Lab 1/")
#loading the dataset
parkinson=read.csv("parkinsons.csv")
#REMOVE UNNECESSARY COLUMNS FIRST
parkinson$subject.=c()
parkinson$sex=c()
parkinson$test_time=c()
parkinson$age=c()
parkinson$total_UPDRS=c()
#choosing training and test sets
set.seed(12345)
n=nrow(parkinson)
id=sample(1:n, floor(n*0.6))
train=parkinson[id,]
test=parkinson[-id,]
#scaling
library(caret)
scaler=preProcess(train)
trainS=predict(scaler,train)
testS=predict(scaler,test)
#Compute a linear regression model from the training data, estimate training
#and test MSE and comment on which variables contribute significantly to the model
#no intercept is needed in the modelling
fit = lm(motor_UPDRS~.-1 , data=trainS)
sum=summary(fit)
#calculate MSE for train dataset
MSEtrain=mean(sum$residuals^2)
#run model on test dataset
fitTest=predict(fit,testS,interval = "prediction")
#calculate MSE for test dataset
MSEtest=mean((testS$motor_UPDRS-fitTest)^2)
#comment on which variables contribute significantly to the model.
# Shimmer.DDA
# Shimmer.APQ3
# we scaled the data => resulting coefficients can be compared and we can compare absolute values
#??????question?????????? (significant contribution) - should some other variables be picked? How to define a threshold?
#answer : look at p-values and ***
#CHECK IN GOOGLE
summary(fit)
#Implement 4 following functions
#3a) Log-likelihood function that for a given parameter vector ğœ½ and
#dispersion ğœ computes the log-likelihood function log ğ‘ƒ(ğ‘‡|ğœ½, ğœ) for
#the stated model and the training data
#Going by that motor_UPDRS is a normal distribution as mentioned in 1.
#Formula log-likelihood F(ğœ½)=log(L(ğœ½))=sum(over i=1 to n) log(fi(yi|ğœ½))
#??????question??????????: is the set of ğœ½we are trying to optimize equal to the vector of coefficients in LR model   - YES
vectorTheta = fit$coefficients
names(vectorTheta) <- NULL
dispersionSigma = summary(fit)$sigma
logLikelihood<-function(theta,sigma){
#population size n
n<-length(theta)
#print(n)
actualValue<-trainS$motor_UPDRS[1:n] #y in the formula.
#print(actualValue)
#print(length(parkinson$motor_UPDRS))
#ERROR!
#Expected value, estimated by using the average value.
#ev=mean(theta)
#changed it to transpose of the in parametr theta to match p44 book.
predictedTheta=theta
#print(ev)
#Done here because when calculations were done in "sum" in the formula, errors appeared.
x=(predictedTheta-actualValue)^2 #is vector of
#print("printing x")
#print(x)
#ERROR!
#Formula
result=((-1*n/2)*log(2*pi,base=exp(1))-(n/2)*log(sigma^2,base=exp(1))-1/(2*sigma^2)*sum(x, na.rm=FALSE))
return (result)
}
print(logLikelihood(vectorTheta,dispersionSigma))
logLik(fit)
#3b) Ridge function that for given vector ğœ½ğœ½, scalar ğœğœ and scalar ğœ†ğœ† uses function from 3a and adds up a Ridge penalty ğœ†â€–ğœ½ğœ½â€–2 to the minus loglikelihood
mylambda=1
myridge<-function(theta,sigma,lambda){
# print(theta)
#print(sigma)
#print(lambda)
ridgePenalty=lambda*sum(theta^2)
#print(ridgePenalty)
# print(logLikelihood(theta,sigma ))
result =  -logLikelihood(theta,sigma )+ ridgePenalty
# print(result)
return (result)
}
print(myridge(vectorTheta,dispersionSigma,mylambda))
# 3c) RidgeOpt function that depends on scalar ğœ† , uses function from 3b
#and function optim() with method=â€BFGSâ€ to find the optimal ğœ½ and ğœ for the given ğœ†.
#ANSWER: initiate with 0 and then compare the result of our custom prediction with true values
par1 <- rep(c(0),each=16)
sigmaPar=1
par1 <- append(par1,sigmaPar)
myRidgeOpt=function(lambdaIn) {
result<- optim(par1,fn=myridge,sigma=sigmaPar,lambda=lambdaIn,method="BFGS")
#WE SHOULD OPTIMIZE THEM TOGETHER
return (result)
}
optimal=myRidgeOpt(mylambda)
print(optimal)
#3d) Df function that for a given scalar ğœ† computes the degrees of freedom
#of the Ridge model based on the training data
install.packages("glmnet")
library(glmnet)
install.packages("psych")
library(psych)
lambdaIn=1
DF=function(lambdaIn) {
# Compute hat-matrix and degrees of freedom
ld <- lambdaIn * diag(ncol(trainS))
trainmatrix<-data.matrix(trainS)
H <- trainmatrix %*% solve((t(trainmatrix) %*% trainmatrix + ld)) %*% t(trainmatrix)
#library("psych")
df <- tr(H)
return(df)
}
print(DF(lambdaIn))
#----------------------------4. -------
#By using function RidgeOpt, compute optimal ğœ½ğœ½ parameters for ğœ†ğœ† = 1, ğœ†ğœ† =
#  100 and ğœ†ğœ† = 1000. Use the estimated parameters to predict the
#motor_UPDRS values for training and test data and report the training and
#test MSE values. Which penalty parameter is most appropriate among the
#selected ones? Compute and compare the degrees of freedom of these models
#and make appropriate conclusions.
#compute
myRidgeOpt(1)$par
myRidgeOpt(100)
myRidgeOpt(1000)
#Predicted motor_UPDRS for train set with lamda=1
data.matrix(trainS[,2:17])%*%myRidgeOpt(1)$par[1:16]
data.matrix(trainS[,2:17])%*%myRidgeOpt(100)$par[1:16]
data.matrix(trainS[,2:17])%*%myRidgeOpt(1000)$par[1:16]
#MSE calculation train set
MSEtrainLambda1 <- mean((data.matrix(trainS[,2:17])%*%myRidgeOpt(1)$par[1:16]-trainS$motor_UPDRS)^2)
MSEtrainLambda100 <- mean((data.matrix(trainS[,2:17])%*%myRidgeOpt(100)$par[1:16]-trainS$motor_UPDRS)^2)
MSEtrainLambda1000 <- mean((data.matrix(trainS[,2:17])%*%myRidgeOpt(1000)$par[1:16]-trainS$motor_UPDRS)^2)
#MSE calculation for test
MSEtestLambda1 <- mean((data.matrix(testS[,2:17])%*%myRidgeOpt(1)$par[1:16]-testS$motor_UPDRS)^2)
MSEtestLambda100 <- mean((data.matrix(testS[,2:17])%*%myRidgeOpt(100)$par[1:16]-testS$motor_UPDRS)^2)
MSEtestLambda1000 <- mean((data.matrix(testS[,2:17])%*%myRidgeOpt(1000)$par[1:16]-testS$motor_UPDRS)^2)
#Pick penalty value lambda 100 it gives the smallest error
#Compute and compare degrees of freedom
DF(1)
DF(100)
DF(1000)
#higher lambda more complex
#higher lambda means fewer independent variables, fewer variables that can vary in value <-
#more degrees of freedom better approximation.
#When we have lambda is 100 error is lowest
#We want low degrees of freedom and the penalty factor lambda high but with higher lambda but
#also low ammounts of errors and after a 100 lambda the errors went up again.
